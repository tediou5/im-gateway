# 服务出口流量过大解决方法技术调研

Author: qians

Date: 2023/5/15

Version: v0.0.1

## 问题

目前在8核16G的服务器上, 处理性能(不推送, 只处理)为1300w. 每条消息平均长度为1K. 因此目标出口流量为13000000k, 约为13G/s. 远远超出了单张网卡处理能力.

## 解决方法

解决方法分为硬件(网卡)配置和代码优化两个方向.

### 挂载多网卡

各服务商提供的云服务器都可以将多张网卡挂载载一个服务器上, 以提高网络IO处理性能和吞吐量. 但这里有一个问题, 普通的配置多网卡不能将多张网卡配置在一个网段(每张网卡一个IP). 大家配置多网卡的一般需求是公网一张网卡处理公网流量, 内网一张网卡处理内部流量. 相互直接并不共通.
这有几个问题:
  1. 每个网卡需要配置一个公网IP. 
  2. 如果配置多个公网IP, 全部挂到Nginx上进行流量的分配, 会导致整个流量分配的算法复杂度急剧上升. 并且与之相关的分流限流算法也会跟着一起收到影响: 无法简单的感知每台服务器的负载量到底是多少, 服务器也无法简单的清楚自身健康状况, 需要对每一张网卡的流量负载都分别进行监控才行. 但这一点不是很容易做到.

### Linux多网卡绑定技术Bond

#### 模式

在linux系统中, 提供了Bond工具. 利用Bond我们可以将多张网卡绑定为一个虚拟网卡, 共同处理一个IP网段的流量.
根据设置的模式的不同, 会有不同的分配和处理流量的行为:

- mode = 0 round-robin

此模式下, 多网卡会进行负载均衡. 流量会以轮训的方式选择一张网卡收发报文. 基于per packet方式发送, 即每张网卡各分配一个数据包, 此模式以非常朴素的方式增加了吞吐量, 且提供了网卡的容错能力.
但有两个缺陷:
  1. 一个连接或者会话的数据包从不同的接口发出的话，中途再经过不同的链路，在客户端很有可能会出现数据包无序到达的问题，而无序到达的数据包需要重新要求被发送，这样网络的吞吐量就会下降
  2. 该模式需要在交换机端上配置聚合口, 在cisco交换机上为: port channel.

- mode = 1 active-backup

此模式为主备模式: 一张网卡为主状态, 承载所有流量, 备状态的网卡不会有任何流量. 只有当主卡down掉时, 才会将备状态的卡切换至主状态.
此模式用于提高系统可用性, 与当前需求无关.

- mode = 2 load-balancing (xor)

此模式多网卡进行负载均衡. 流量通过目标源的mac和hash因子做xor(异或)算法来分配到一个固定的网卡. 此模式也可以满足我们的需求.
但此模式有2个缺陷:

  1. 该模式需要在交换机端上配置聚合口, 在cisco交换机上为: port channel.
  2. 若使用了代理, 例如nginx, 则目标源地址都是nginx服务器的地址, 那么所有流量依然会在同个网卡上, 无法达到负载均衡扩大吞吐量的效果.

- mode = 3 fault-tolerance

此模式下, 数据会进行广播, 即每个网卡都会冗余发送这份数据. 此模式更多用于金融系统中保证必达, 与我们的需求不符. 不考虑.

- mode = 4 802.3ad

创建一个聚合组，它们共享同样的速率和双工设定。根据802.3ad协议将多个slave工作在同一个激活的聚合体下。外出流量的slave选举是基于传输hash策略，该策略可以通过xmit_hash_policy选项从缺省的XOR策略改变到其他策略。 但使用该模式有3个权限:
  1. ethtool支持获取每个slave的速率和双工设定
  2. switch(交换机)支持IEEE802.3ad Dynamic link aggregation
  3. 大多数switch(交换机)需要经过特定配置才能支持802.3ad模式

- mode = 5 balance-tlb
 
这种模式相较mode2异或策略及mode4 LACP模式的hash策略相对智能, 会主动根据对端的MAC地址上的流量, 智能的分配流量从哪个网卡发出. 但不足之处在于, 仍使用一块网卡接收数据. 存在的问题与load balancing(xor)也是一样的一样, 如果对端MAC地址是唯一的, 那么策略就会失效. 这个模式下bond成员使用各自的mac, 而不是上面几种模式是使用bond0接口的mac. 优势是此模式无需交换机支持.

- mode = 6 adaptive load balancing

该模式除了balance-tlb模式的功能外, 同时加上针对IPV4流量接收的负载均衡. 接收负载均衡是通过ARP协商实现的. 在进行ARP协商的过程中, bond模块将对端和本地的mac地址进行绑定, 这样从同一端发出的数据, 在本地也会一直使用同一块网卡来接收. 若是网络上发出的广播包, 则由不同网卡轮询的方式来进行接收. 通过这种方式实现了接收的负载均衡. 该模式同样无需交换机支持.

#### 总结

mode5和mode6不需要交换机端的设置, 网卡能自动聚合. mode4需要支持802.3ad协议. mode0, mode2和mode3理论上需要配置交换机.

我会首先尝试配置使用mode6模式, 看能否满足需求.

### 代码优化

#### 压缩

既然出口流量过大, 那么最直接的优化就是使用各自压缩算法降低出口流量. 经过初步调研, 我倾向于使用压缩率最大的zstd压缩算法. 详见压缩技术选型.

#### 操作系统中网络调用

本系统实际上自己能控制的就是处理的逻辑和行为, 之后的发送其实都是调用相关操作系统的方法调用, 并等待操作系统的通知再继续推进.
这里可以尝试使用在最新的linux系统内核中提供的新的异步IO系统: io_uring. 来获得更高的性能.
缺陷: 
  1. 仅能在Linux的高版本中使用.
  2. io_uring是单独的接口和特殊的调用方式, 对代码是侵入式的, 需要修改底层系统调用部分来切换至io_uring.