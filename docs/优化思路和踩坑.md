# TCP推送服务优化思路和踩坑

Author: qians

Version: 0.1.0

Date: 2023/5/12

## 服务架构演变

### 基础架构

最开始时候其实没想太深, 主要思路就是运用Actor思想去除锁. 所有事件收集到一个EventLoop中统一处理, 每个连接放在自己的异步任务中, 丢一个channel发送端给EventLoop.
EventLoop独占这些连接进行统一管理, 避免了多线程竞争和数据共享.
之后虽然做了大量的优化, 但其实基于基础架构并没有变.

![基础架构](./基础架构.png "基础架构")

代码实现也很简单, 其实我一直觉得, 基于Channel的Actor模型比加锁在可读性和可维护性上高出太多了.

- 运行

```rust
tokio::task::spawn_local(async move {
    event_loop::run().await;
})
```

- 大致逻辑

```rust
use tokio_stream::StreamExt as _;

// 连接
let mut users: HashMap<String /* Pin */, Connection>;

// 群缓存
let mut chats: ahash::AHashMap<String  /* chat */, Vec<Pin>>;

// 从队列中不停的读事件
while let Some(event) = event_collect_rx.next().await {
    // 处理事件, 因为这是同步的在处理任务, 所以可以直接将资源的可变引用传递下去, 不需要加锁.
    if let Err(e) = process(&mut users, &mut chats, event).await {
        // TODO: handle error
        tracing::error!("preocess event error: {e}");
    };
}

// 事件
#[derive(Debug, serde_derive::Serialize, serde_derive::Deserialize)]
pub(crate) enum Event {
    #[serde(skip)]
    Login(
        String,                      /* pin */
        Vec<std::sync::Arc<String>>, /* chats */
        crate::linker::Platform,
    ),
    Private(
        std::collections::HashSet<String>, /* recvs */
        #[serde(with = "hex")] Vec<u8>,
    ),
    Group(
        String,                            /* chat */
        std::collections::HashSet<String>, /* exclusions */
        std::collections::HashSet<String>, /* additional */
        #[serde(with = "hex")] Vec<u8>,
    ),
    Chat(chat::Action),
}
```

对于向EventLoop发送只需要向对应的mailbox发消息就行
```rust
if let Err(error) = dispatcher.send(event).await {
    // Handle error.
};
```

### 小细节优化

细节上面的优化挺多的, 也挺琐碎的, 归纳来说都是以下两点:

- 重复的数据用Arc/Rc智能指针包起来来共享, 多地存一份, 减少冗余. 例如chats缓存中的pin, 多个chat中可能存在大量重复的人. 发送的消息, 在Channel中的时候也可以用Arc包起来, 最终写入TCP时才copy出来等等.
- 减少不必要Clone, 例如Kafka消费来的数据, 不进行编解码直接将Bytes推送. Tcp读到的消息只在第一次时解码进行鉴权, 成功之后的所有消息都直接生产到Kafka.

### 架构升级

当前架构主要的问题其实是控制度不够, 因为我所有任务都是交给Tokio(异步运行时)进行调度的. 而默认的多线程Tokio会将任务跨线程调度以均衡cpu的使用. 这就带来了额外的限制(T: Send + Sync), 以及线程切换的额外开销. 其次是任务在cpu多核切换时, 操作系统也有额外的上下文切换开销, 也加大了CPU缓存失效的概率. 架构调整也主要时基于此. 

- EventLoop固定在绑定cpu的线程本地, 并拥有一个唯一的编号`processor-event-loop-{core_id}`.
- Kafka和Dispatcher(调度器)共用一个线程固定在一个核心上.
- 其余的每个核上都绑定一个EventLoop.
- 用户根据pin计算ConHash被Dispatcher分配到固定编号的EventLoop上.
- Kafka消费到事件后首先发往Dispatcher, 然后被Dispatcher调度到对应编号的EventLoop.
  - 私聊消息直接计算出编号并单发.
  - 群相关消息广播至所有EventLoop.
  - 每个EventLoop管理的TCP连接也固定在同一线程本地.

![基础架构](./优化架构.png "优化架构")

```rust
pub async fn run(core_ids: Vec<core_affinity::CoreId>) -> anyhow::Result<()> {
    let (collect_tx, collect_rx) = tokio::sync::mpsc::channel::<Event>(20480);
    let mut collect_rx = tokio_stream::wrappers::ReceiverStream::new(collect_rx);
    crate::DISPATCHER.set(collect_tx).unwrap();

    // save every event_loop
    let mut event_loops = vec![];
    let mut conhash = conhash::ConsistentHash::new();
    for id in core_ids.into_iter() {
        let event_loop: event_loop::EventLoop = event_loop::run(id);
        conhash.add(&event_loop, 3);
        event_loops.push(event_loop);
    }

    use tokio_stream::StreamExt as _;

    while let Some(event) = collect_rx.next().await {
        dispatch(&conhash, &event_loops, event).await?;
    }
    Ok(())
}

```